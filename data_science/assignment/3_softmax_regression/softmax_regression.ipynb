{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"h03_김태산_20143211.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN6TJK6zR8rgOXvTla3Hnrr"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YrT7aMl5LN-2"},"source":["# L07.1 Softmax Regression with Pytorch\n","---\n","이름: 김태산\n","\n","학과: 자동차공학과\n","\n","학번: 20143211\n","\n","---\n","### Pytorch로 Softmax Regression 구현"]},{"cell_type":"code","metadata":{"id":"rjTAIlXnKJV_","executionInfo":{"status":"ok","timestamp":1602431186500,"user_tz":-540,"elapsed":984,"user":{"displayName":"­김태산[자동차공학과]","photoUrl":"","userId":"10967533091290920657"}}},"source":["import torch\n","# 학습 데이터 생성\n","x_train = torch.FloatTensor([ [1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5],\n","                             [1,2,5,6], [1,6,6,6], [1,7,7,7] ])\n","y_train = torch.FloatTensor([ [0,0,1], [0,0,1], [0,0,1], [0,1,0], [0,1,0], [0,1,0],\n","                             [1,0,0], [1,0,0] ])"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"J_qXRpajM95x","executionInfo":{"status":"ok","timestamp":1602431198697,"user_tz":-540,"elapsed":1486,"user":{"displayName":"­김태산[자동차공학과]","photoUrl":"","userId":"10967533091290920657"}},"outputId":"6654c252-6446-4904-9cd8-0b1cf2587151","colab":{"base_uri":"https://localhost:8080/","height":209}},"source":["# W, b 초기화\n","W = torch.zeros(4, 3, requires_grad=True)\n","b = torch.zeros(1, 3, requires_grad=True)\n","\n","# Optimizer 생성\n","optimizer = torch.optim.Adam([W, b], lr=0.1)\n","\n","# 반복횟수 설정\n","for epoch in range(3001):\n","  # 가설 및 비용 설정\n","  hypothesis = torch.softmax(torch.mm(x_train, W) + b, dim=1) # dim 파라미터는 softmax 값이 계산될 차원을 의미한다.\n","  cost = -torch.mean(torch.sum(y_train * torch.log(hypothesis), dim=1))\n","  \n","  # Optimizer를 이용한 경사 계산 및 W, b 업데이트\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","\n","  # 학습이 잘 되는지 확인하기 위한 내용 출력\n","  if epoch % 300 == 0:\n","    print(\"epoch: {}, cost: {:.6f}\".format(epoch, cost.item()))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["epoch: 0, cost: 1.098612\n","epoch: 300, cost: 0.105263\n","epoch: 600, cost: 0.042634\n","epoch: 900, cost: 0.023111\n","epoch: 1200, cost: 0.014479\n","epoch: 1500, cost: 0.009879\n","epoch: 1800, cost: 0.007124\n","epoch: 2100, cost: 0.005338\n","epoch: 2400, cost: 0.004113\n","epoch: 2700, cost: 0.003236\n","epoch: 3000, cost: 0.002588\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W0DQ_mQhRghF"},"source":["- Pytorch를 이용하여 Softmax regression을 직접 구현해보았다.\n","- 출력 결과를 보면, 훈련이 진행될수록 cost 값이 줄어드는 것으로 보아 훈련이 올바르게 진행되고 있음을 알 수 있다."]},{"cell_type":"code","metadata":{"id":"KpzMFWL0QfMa","executionInfo":{"status":"ok","timestamp":1602435118050,"user_tz":-540,"elapsed":933,"user":{"displayName":"­김태산[자동차공학과]","photoUrl":"","userId":"10967533091290920657"}},"outputId":"02acf4a7-dff1-4db4-f304-1690ab5f1f5d","colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["# x가 [1,11,10,9], [1,3,4,3], [1,1,0,1] 일 때, y값은?\n","x_test = torch.FloatTensor([ [1,11,10,9], [1,3,4,3], [1,1,0,1] ])\n","test_all = torch.softmax(torch.mm(x_test, W) + b, dim=1)\n","print(test_all)\n","print(torch.argmax(test_all, dim=1))"],"execution_count":41,"outputs":[{"output_type":"stream","text":["tensor([[1.0000e+00, 5.5163e-19, 7.0149e-38],\n","        [1.4800e-02, 7.4294e-01, 2.4226e-01],\n","        [1.2256e-33, 9.0835e-12, 1.0000e+00]], grad_fn=<SoftmaxBackward>)\n","tensor([0, 1, 2])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VbrjiTWQRxk-"},"source":["- test_all의 출력 결과를 보면 테스트 샘플 별로 softmax 함수를 통해 각 클래스에 대한 확률값이 출력되는 것을 알 수 있다.\n","- 테스트 샘플로 테스트한 결과 첫번째 샘플(x = [1,11,10,9])은 첫번째 클래스로, 두번째 샘플(x = [1,3,4,3])은 두번째 클래스로, 세번째 샘플(x = [1,1,0,1])은 세번째 클래스로 예측값을 출력한다."]},{"cell_type":"markdown","metadata":{"id":"rhGLiq4DSqtc"},"source":["### 조금 더 깔끔하게 Softmax"]},{"cell_type":"code","metadata":{"id":"YOJ1GuU8S_dR","executionInfo":{"status":"ok","timestamp":1602434564991,"user_tz":-540,"elapsed":1476,"user":{"displayName":"­김태산[자동차공학과]","photoUrl":"","userId":"10967533091290920657"}},"outputId":"fc043d04-4d44-41e5-be13-181806dca49e","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# torch.nn.functional을 F라는 이름으로 사용하겠다고 선언\n","# torch.nn.functional 모듈에는 이번 실습에서 사용할 cross_entropy() 뿐만 아니라,\n","# 다양한 손실함수와 활성함수, 인공신경망을 만드는데 사용되는 함수들을 포함하고 있다.\n","import torch.nn.functional as F\n","\n","# y_train 수정\n","# F.cross_entropy를 사용할 때는 y_train의 값을 클래스 번호로 입력할 수 있다.\n","y_train = torch.LongTensor([2,2,2,1,1,1,0,0])\n","y_train"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([2, 2, 2, 1, 1, 1, 0, 0])"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"L1CTejmGTSgW","executionInfo":{"status":"ok","timestamp":1602434567196,"user_tz":-540,"elapsed":2192,"user":{"displayName":"­김태산[자동차공학과]","photoUrl":"","userId":"10967533091290920657"}},"outputId":"d25ae8b6-197d-47a9-bc65-7fbade909eb9","colab":{"base_uri":"https://localhost:8080/","height":209}},"source":["W = torch.zeros(4, 3, requires_grad=True)\n","b = torch.zeros(1, 3, requires_grad=True)\n","\n","optimizer = torch.optim.Adam([W, b], lr=0.1)\n","\n","for epoch in range(3001):\n","  # 가설, 비용 수정\n","  z = torch.mm(x_train, W) + b\n","  cost = F.cross_entropy(z, y_train) # 주의! F.cross_entropy는 softmax와 cross entropy를 합친 것.\n","                                     # input 위치(z)에는 N*C 크기의 텐서(N: 데이터 갯수, C: 클래스 갯수)를,\n","                                     # target 위치(y_train)에는 N 크기의 클래스를 나타내는 텐서를 입력한다.\n","\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","\n","  if epoch % 300 == 0:\n","    print(\"epoch: {}, cost: {:.6f}\".format(epoch, cost.item()))"],"execution_count":33,"outputs":[{"output_type":"stream","text":["epoch: 0, cost: 1.098612\n","epoch: 300, cost: 0.105263\n","epoch: 600, cost: 0.042634\n","epoch: 900, cost: 0.023111\n","epoch: 1200, cost: 0.014479\n","epoch: 1500, cost: 0.009879\n","epoch: 1800, cost: 0.007124\n","epoch: 2100, cost: 0.005338\n","epoch: 2400, cost: 0.004113\n","epoch: 2700, cost: 0.003236\n","epoch: 3000, cost: 0.002588\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I822ErmAZy2_"},"source":["- 출력 결과를 보면, 위에서 Softmax regression을 직접 구현한 결과와 동일한 결과를 나타냄을 알 수 있다."]},{"cell_type":"code","metadata":{"id":"ZapZX-JtiIsh","executionInfo":{"status":"ok","timestamp":1602434588129,"user_tz":-540,"elapsed":569,"user":{"displayName":"­김태산[자동차공학과]","photoUrl":"","userId":"10967533091290920657"}},"outputId":"129a347a-2c99-4eb7-e894-7117247d945a","colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["# x가 [1,11,10,9], [1,3,4,3], [1,1,0,1] 일 때, y값은?\n","x_test = torch.FloatTensor([ [1,11,10,9], [1,3,4,3], [1,1,0,1] ])\n","z = torch.mm(x_test, W) + b\n","test_all = torch.softmax(z, dim=1)\n","print(test_all)\n","print(torch.argmax(test_all, dim=1))"],"execution_count":35,"outputs":[{"output_type":"stream","text":["tensor([[1.0000e+00, 5.5163e-19, 7.0149e-38],\n","        [1.4800e-02, 7.4294e-01, 2.4226e-01],\n","        [1.2256e-33, 9.0835e-12, 1.0000e+00]], grad_fn=<SoftmaxBackward>)\n","tensor([0, 1, 2])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UXk2vsgUiIso"},"source":["- test_all의 출력 결과를 보면 테스트 샘플 별로 softmax 함수를 통해 각 클래스에 대한 확률값이 출력되는 것을 알 수 있다.\n","- 테스트 샘플로 테스트한 결과도 마찬가지로 첫번째 샘플(x = [1,11,10,9])은 첫번째 클래스로, 두번째 샘플(x = [1,3,4,3])은 두번째 클래스로, 세번째 샘플(x = [1,1,0,1])은 세번째 클래스로 예측값을 출력한다."]},{"cell_type":"code","metadata":{"id":"AWFeAmZBZ-3f","executionInfo":{"status":"ok","timestamp":1602434634430,"user_tz":-540,"elapsed":1349,"user":{"displayName":"­김태산[자동차공학과]","photoUrl":"","userId":"10967533091290920657"}},"outputId":"91fc9d5a-f061-4c0e-b5ed-425c5b338aa8","colab":{"base_uri":"https://localhost:8080/","height":209}},"source":["# 어차피 맨날 쓰는 W와 b. nn.Linear로 대체하자!\n","# nn.Linear: x^Tw + b를 간단히 표현하는 방법\n","# torch.nn는 그래프 연산을 위한 기본 구성 요소들을 포함하고 있다.\n","# 예를 들어, torch.nn.Linear는 x^Tw + b과 같은 선형 연산을 수행해준다.\n","import torch.nn as nn\n","\n","model = nn.Linear(4, 3) # 첫번째 정수는 입력할 데이터(x_train)의 크기를,\n","                        # 두번째 정수는 출력할 데이터(클래스 갯수)의 크기를 입력한다.\n","                        # 세번째 파라미터로 bias가 있는데 default값은 True이다.\n","                        # bias=False로 지정할 경우 편향(b) 항을 추가하지 않는다.\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=1)\n","\n","for epoch in range(3001):\n","  z = model(x_train)\n","  cost = F.cross_entropy(z, y_train)\n","\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","\n","  if epoch % 300 == 0:\n","    print(\"epoch: {}, cost: {:.6f}\".format(epoch, cost.item()))"],"execution_count":36,"outputs":[{"output_type":"stream","text":["epoch: 0, cost: 1.547660\n","epoch: 300, cost: 0.022227\n","epoch: 600, cost: 0.008657\n","epoch: 900, cost: 0.004653\n","epoch: 1200, cost: 0.002914\n","epoch: 1500, cost: 0.001991\n","epoch: 1800, cost: 0.001439\n","epoch: 2100, cost: 0.001080\n","epoch: 2400, cost: 0.000833\n","epoch: 2700, cost: 0.000657\n","epoch: 3000, cost: 0.000526\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QmJx4U-TfUEI"},"source":["- 출력 결과를 보면, 이번에는 학습률을 1로 하였기 때문에 Softmax regression을 직접 구현한 결과와 동일한 결과는 아니지만 여전히 cost 값은 훈련이 진행될수록 줄어드는 것으로 보아 훈련이 올바르게 진행되고 있음을 알 수 있다."]},{"cell_type":"code","metadata":{"id":"Rx2Ov3NNi_Z-","executionInfo":{"status":"ok","timestamp":1602434888327,"user_tz":-540,"elapsed":964,"user":{"displayName":"­김태산[자동차공학과]","photoUrl":"","userId":"10967533091290920657"}},"outputId":"0a3fc023-6fb7-45ad-f708-4410bb054133","colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["# x가 [1,11,10,9], [1,3,4,3], [1,1,0,1] 일 때, y값은?\n","x_test = torch.FloatTensor([ [1,11,10,9], [1,3,4,3], [1,1,0,1] ])\n","z = model(x_test)\n","test_all = torch.softmax(z, dim=1)\n","print(test_all)\n","print(torch.argmax(test_all, dim=1))"],"execution_count":38,"outputs":[{"output_type":"stream","text":["tensor([[1.0000e+00, 8.1656e-26, 0.0000e+00],\n","        [7.4271e-07, 7.0770e-01, 2.9230e-01],\n","        [1.4013e-45, 5.8471e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward>)\n","tensor([0, 1, 2])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aFwc2_cei_aH"},"source":["- test_all의 출력 결과를 보면 테스트 샘플 별로 softmax 함수를 통해 각 클래스에 대한 확률값이 출력되는 것을 알 수 있다.\n","- 테스트 샘플로 테스트한 결과도 마찬가지로 첫번째 샘플(x = [1,11,10,9])은 첫번째 클래스로, 두번째 샘플(x = [1,3,4,3])은 두번째 클래스로, 세번째 샘플(x = [1,1,0,1])은 세번째 클래스로 예측값을 출력한다.\n","- 위와 같이 Pytorch의 다양한 함수들을 사용하여 직접 Softmax regression을 구현한 것과 동일한 과정을 좀 더 간단한 방법으로 구현할 수 있었다. "]}]}